{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_app.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"p_n5YSmwgcjo","colab_type":"code","outputId":"144a8df6-8471-47ec-ea48-f33abf39fe97","executionInfo":{"status":"ok","timestamp":1567567444957,"user_tz":-480,"elapsed":60475,"user":{"displayName":"吴绍武","photoUrl":"","userId":"06906366776224872806"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PCXT7Q04xRGR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-5-NVafgsYU","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import time\n","import datetime\n","from tqdm import tqdm\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","#from sklearn.cross_validation import StratifiedKFold\n","from sklearn import metrics\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import RidgeClassifier\n","import jieba\n","#import fool\n","import re\n","from scipy.sparse import csr_matrix, hstack\n","from sklearn.model_selection import StratifiedKFold"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6V7Xvvmng3zQ","colab_type":"code","outputId":"32f25921-a9f3-49e8-b18d-6a3de08e9110","executionInfo":{"status":"ok","timestamp":1567567498998,"user_tz":-480,"elapsed":114497,"user":{"displayName":"吴绍武","photoUrl":"","userId":"06906366776224872806"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["path_drive='/content/drive/My Drive/Colab Notebooks/'\n","final_apptype_train= pd.read_csv(path_drive+\"final_apptype_train.dat\",header=None,encoding='utf8',delimiter='\\t')\n","final_apptype_train.columns=['id','conment','label']\n","appname_package=pd.read_csv(path_drive+\"appname_package.dat\",header=None,encoding='utf8',delimiter='\\t')\n","appname_package.columns=['id','name','conment']\n","\n","apptype_train= pd.read_csv(path_drive+\"apptype_train.dat\",header=None,encoding='utf8',delimiter=' ')\n","#以tab键分割，不知道为啥delimiter='\\t'会报错，所以先读入再分割。\n","apptype_train=pd.DataFrame(apptype_train[0].apply(lambda x:x.split('\\t')).tolist(),columns=['id','label','conment1'])\n","apptype_train['len']=apptype_train['conment1'].apply(lambda x:len(x))\n","#apptype_train=apptype_train[apptype_train['len']>10]\n","\n","train= pd.read_csv(path_drive+\"new_train.csv\")\n","train=train[['conment1','label']]\n","#train=train[train.conment1!='0'].reset_index(drop=True)\n","\n","test= pd.read_csv(path_drive+\"new_test.csv\")\n","test=test[['id','conment1']]\n","\n","##如果conment1为0，用app包名代替\n","for i in tqdm(range(train.shape[0])):\n","    if train.loc[i,'conment1']=='0':\n","        train.loc[i,'conment1']=str(final_apptype_train.loc[i,'id'])\n","for i in tqdm(range(test.shape[0])):\n","    if test.loc[i,'conment1']=='0':\n","        test.loc[i,'conment1']=str(appname_package.loc[i,'name'])\n","\n","train=pd.concat([train,apptype_train[['conment1','label']]],axis=0).reset_index(drop=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 20000/20000 [00:01<00:00, 15513.56it/s]\n","100%|██████████| 80000/80000 [00:47<00:00, 1680.41it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4SWxSIkXg4pJ","colab_type":"code","outputId":"595444f8-8580-4fe1-d884-f5c30fd81712","executionInfo":{"status":"ok","timestamp":1567567498999,"user_tz":-480,"elapsed":114491,"user":{"displayName":"吴绍武","photoUrl":"","userId":"06906366776224872806"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#========================以|为分隔符，把标签分割：===============================\n","train['label1']=train['label'].apply(lambda x:x.split('|')[0])\n","train['label2']=train['label'].apply(lambda x:x.split('|')[1] if '|' in x else x) ##第二个标签有些没有，此处补0\n","print('训练集第一个标签分布：',train.label1.value_counts())\n","##去掉样本少于5个的类别,（主要考虑到后续的5折交叉验证）：\n","train=train[~train.label1.isin(['140805','142306','140110','142309','141703','140105'])].reset_index(drop=True)\n","\n","from sklearn import preprocessing\n","lbl = preprocessing.LabelEncoder()\n","lbl.fit(train['label1'].values)\n","train['label1'] = lbl.transform(train['label1'].values)\n","num_class=train['label1'].max()+1\n","print('num_class',num_class)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["训练集第一个标签分布： 140901    8454\n","140206    4004\n","141001    2036\n","142103    1575\n","140701    1575\n","140207    1488\n","140210    1140\n","140211    1129\n","142302    1103\n","140404    1047\n","140601     998\n","142104     968\n","140402     956\n","140205     747\n","140212     735\n","140304     724\n","140209     689\n","140214     646\n","140704     634\n","142308     632\n","140301     607\n","140604     589\n","140603     582\n","140202     572\n","140106     542\n","142402     532\n","140112     492\n","142701     491\n","140111     487\n","142105     487\n","          ... \n","140303      75\n","140411      75\n","140405      74\n","140804      73\n","140410      73\n","140606      71\n","142305      70\n","140408      70\n","141301      67\n","142304      65\n","140308      63\n","140306      60\n","140703      59\n","142403      58\n","140407      52\n","140806      52\n","140412      48\n","141701      35\n","141802      30\n","140607      25\n","140808      21\n","140809      19\n","140810      19\n","140108      17\n","142309      12\n","142306      11\n","141703       9\n","140110       6\n","140805       5\n","140105       1\n","Name: label1, Length: 125, dtype: int64\n","num_class 119\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"exkNlZmTjBV-","colab_type":"code","outputId":"d8638228-6f0d-4ec7-db29-2933da8dc536","executionInfo":{"status":"ok","timestamp":1567567514798,"user_tz":-480,"elapsed":130281,"user":{"displayName":"吴绍武","photoUrl":"","userId":"06906366776224872806"}},"colab":{"base_uri":"https://localhost:8080/","height":973}},"source":["pip install keras-bert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting keras-bert\n","  Downloading https://files.pythonhosted.org/packages/fa/c3/dd1f3c177cc0d50f95729a7e1f050daaadd918c851c84f335f1c1b63c769/keras-bert-0.77.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.16.4)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.2.5)\n","Collecting keras-transformer>=0.30.0 (from keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/83/4c/972325395b38547df8a74be89e980922c1dc9f921cc2eb613e086c6bc632/keras-transformer-0.30.0.tar.gz\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.8.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.3.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n","Collecting keras-pos-embd>=0.10.0 (from keras-transformer>=0.30.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.22.0 (from keras-transformer>=0.30.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n","Collecting keras-layer-normalization>=0.12.0 (from keras-transformer>=0.30.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/ea/f3/a92ce51219280eea003911722046db17eaebf5f26679a73887a5c357abe4/keras-layer-normalization-0.13.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.5.0 (from keras-transformer>=0.30.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.7.0 (from keras-transformer>=0.30.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n","Collecting keras-self-attention==0.41.0 (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n","Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.77.0-cp36-none-any.whl size=37761 sha256=d416dbe64091b66aacdfcf87b767ba994ba7ca476d351471644b157b498c3286\n","  Stored in directory: /root/.cache/pip/wheels/54/4f/83/b0a44df38a87c9d50cd15340d51eec24b5fb54637d86fd0352\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.30.0-cp36-none-any.whl size=13388 sha256=138c9cc97ef3ab7c58e2af79dda02c36061c351d10e61e18600badd92c879143\n","  Stored in directory: /root/.cache/pip/wheels/b5/06/e3/172763eea3a0b3046c91a75ec778c54e55f96ee0efdb79c044\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=21dc1fbd5da9b09e821c58082f779a6817e0b716f638107698ba0a333c844b0e\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=e424a4df8a2287099fd18f0dccc0755ede8b4d2ea1e123b254703aa51248cdde\n","  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.13.0-cp36-none-any.whl size=5209 sha256=cb10180fbb86254bccb56c209f6455ea2a45783949a0eb4d30d7bf2434835a7d\n","  Stored in directory: /root/.cache/pip/wheels/50/2b/71/d1d06f71d78c46a9912dc89a5bb46f357cf64fa05883fadc64\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=a45cd5aa8166268aadcf96332285787076b1abf2a79b82eed67d316b05c3974c\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=c33491bdeae12e281ce8eae7372b2fadb5a31601738b9d305b5fe49f662e3dbb\n","  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=704511a33dd01f9cbef2a40b4f217b10290e7296d658b8564e9e573a7a2213d1\n","  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n","Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n","Successfully installed keras-bert-0.77.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.13.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.30.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aqXmt49KhUZI","colab_type":"code","outputId":"1b70e2c2-306e-48bf-87e1-2d5aa8fb51eb","executionInfo":{"status":"ok","timestamp":1567567517720,"user_tz":-480,"elapsed":133200,"user":{"displayName":"吴绍武","photoUrl":"","userId":"06906366776224872806"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import json\n","import pandas as pd\n","from random import choice\n","from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n","import re, os\n","import codecs\n","from keras.utils.np_utils import to_categorical\n","import gc\n","gc.collect()\n","maxlen = 300\n","config_path =path_drive+'chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json'\n","checkpoint_path =path_drive+'chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt'\n","dict_path =path_drive+'chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/vocab.txt'\n","\n","token_dict = {}\n","with codecs.open(dict_path, 'r', 'utf8') as reader:\n","    for line in reader:\n","        token = line.strip()\n","        token_dict[token] = len(token_dict)\n","\n","class OurTokenizer(Tokenizer):\n","    def _tokenize(self, text):\n","        R = []\n","        for c in text:\n","            if c in self._token_dict:\n","                R.append(c)\n","            elif self._is_space(c):\n","                R.append('[unused1]') # space类用未经训练的[unused1]表示\n","            else:\n","                R.append('[UNK]') # 剩余的字符是[UNK]\n","        return R\n","\n","tokenizer = OurTokenizer(token_dict)\n","\n","def creat_data(data):\n","    X1, X2, Y = [], [], []\n","    indexs=[]\n","    for i in range(len(data)):\n","        d =data.loc[i,'conment1']\n","        text = d[:maxlen]\n","        x1, x2 = tokenizer.encode(first=text)\n","        y = data.loc[i,'label1']\n","        if data.loc[i,'label2']!=-99:\n","          indexs.append([i,data.loc[i,'label2']])\n","        X1.append(x1)\n","        X2.append(x2)\n","        Y.append([y])\n","    X1 = seq_padding(X1)\n","    X2 = seq_padding(X2)\n","    Y = seq_padding(Y)\n","    z=to_categorical(Y)\n","    return [X1,X2],z\n","\n","def creat_data_test(data):\n","    X1, X2= [], []\n","    for i in range(len(data)):\n","        d =data.loc[i,'conment1']\n","        text = d[:maxlen]\n","        x1, x2 = tokenizer.encode(first=text)\n","        X1.append(x1)\n","        X2.append(x2)\n","    X1 = seq_padding(X1)\n","    X2 = seq_padding(X2)\n","    return [X1,X2]\n","\n","def seq_padding(X, padding=0):\n","    L = [len(x) for x in X]\n","    ML = max(L)\n","    return np.array([\n","        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n","    ])\n","\n","\n","class data_generator:\n","    def __init__(self, data, batch_size=10):\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.steps = len(self.data) // self.batch_size\n","        if len(self.data) % self.batch_size != 0:\n","            self.steps += 1\n","    def __len__(self):\n","        return self.steps\n","    def __iter__(self):\n","        while True:\n","            idxs = range(len(self.data))\n","            #np.random.shuffle(idxs)\n","            #X1, X2, Y = [], [], []\n","            X1, X2= [], []\n","            for i in idxs:\n","                d = self.data.loc[i,'conment1']\n","                text = d[:maxlen]\n","                x1, x2 = tokenizer.encode(first=text)\n","                #y = self.data.loc[i,'label1']\n","                X1.append(x1)\n","                X2.append(x2)\n","                #Y.append([y])\n","                if len(X1) == self.batch_size or i == idxs[-1]:\n","                #if i == idxs[-1]:\n","                    X1 = seq_padding(X1)\n","                    X2 = seq_padding(X2)\n","                    #Y = seq_padding(Y)\n","                    yield [X1, X2]#, to_categorical(Y)\n","                    #[X1, X2, Y] = [], [], []\n","                    X1, X2= [], []\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Vn6lpU7PhiUZ","colab_type":"code","outputId":"65f78231-8745-4d8c-81c3-7616fdb17103","colab":{"base_uri":"https://localhost:8080/"}},"source":["from keras.layers import *\n","from keras.models import Model\n","import keras.backend as K\n","from keras.optimizers import Adam\n","\n","\n","bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n","\n","for l in bert_model.layers:\n","    l.trainable = True\n","\n","x1_in = Input(shape=(None,))\n","x2_in = Input(shape=(None,))\n","\n","x = bert_model([x1_in, x2_in])\n","x = Lambda(lambda x: x[:, 0])(x)\n","p = Dense(num_class, activation='softmax')(x)\n","\n","model = Model([x1_in, x2_in], p)\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer=Adam(1e-5), # 用足够小的学习率\n","    metrics=['accuracy']\n",")\n","model.summary()\n","\n","column='conment1'\n","label=train.label1.values\n","\n","dtest=creat_data_test(test)\n","N=5\n","stack_train = np.zeros((train.shape[0],num_class))\n","stack_test = np.zeros((test.shape[0],num_class))\n","skf=StratifiedKFold(n_splits=N,shuffle=True,random_state=42)\n","for i,(tr,va) in enumerate(skf.split(train[column],label)):\n","      \n","#for i, (tr, va) in enumerate(StratifiedKFold(label, n_folds=n_folds, random_state=42)):\n","    print('stack:%d/%d' % ((i + 1), N))\n","    #train_D = data_generator(traindata.loc[tr].reset_index(drop=True))\n","    #valid_D = data_generator(traindata.loc[va].reset_index(drop=True))\n","\n","    train_D = creat_data(train.loc[tr].reset_index(drop=True))\n","    valid_D = creat_data(train.loc[va].reset_index(drop=True))\n","    if not os.path.exists(path_drive+'bert_model_%d.h5'%i):\n","        model.fit(train_D[0], train_D[1],\n","                     validation_data=(valid_D[0], valid_D[1]),\n","                     epochs=1,\n","                     batch_size=10,\n","                     shuffle=True,\n","                     )\n","    else:\n","        model.load_weights(path_drive+'bert_model_%d.h5'%i)\n","    #dtest=data_generator(test)\n","    score_te = model.predict(dtest)\n","    score_va = model.predict(valid_D[0])\n","    \n","    stack_train[va] += score_va\n","    stack_test += score_te\n","    \n","print(\"model acc_score:\",metrics.accuracy_score(label,np.argmax(stack_train,axis=1),\\\n","                                                normalize=True, sample_weight=None))\n","\n","##获取第一第二个标签：取概率最大的前两个即可：\n","m=pd.DataFrame(stack_train)\n","first=[]\n","second=[]\n","for j,row in m.iterrows():\n","    zz=list(np.argsort(row))\n","    first.append(row.index[zz[-1]]) ##第一个标签\n","    second.append(row.index[zz[-2]]) ##第二个标签\n","m['label1']=first\n","m['label2']=second\n","\n","#计算准确率，只要命中一个就算正确：\n","k=0\n","for i in range(len(label)):\n","    if label[i] in [m.loc[i,'label1'],m.loc[i,'label2']]:\n","        k+=1\n","    else:\n","        pass\n","print('线下准确率：%f'%(k/len(label)))\n","\n","##准备测试集结果：\n","results=pd.DataFrame(stack_test)\n","first=[]\n","second=[]\n","for j,row in results.iterrows():\n","    zz=list(np.argsort(row))\n","    first.append(row.index[zz[-1]]) ##第一个标签\n","    second.append(row.index[zz[-2]]) ##第二个标签\n","results['label1']=first\n","results['label2']=second\n","##之前编码，最后逆编码回来：\n","\n","try:\n","    results['label1']=results['label1'].apply(lambda x:lbl.inverse_transform(int(x)))\n","    results['label2']=results['label2'].apply(lambda x:lbl.inverse_transform(int(x)))\n","except:\n","    results['label1']=lbl.inverse_transform(results['label1'])\n","    results['label2']=lbl.inverse_transform(results['label2'])\n","##结合id列，保存：\n","pd.concat([test[['id']],results[['label1','label2']]],axis=1).to_csv(path_drive+'submit2.csv',index=None,encoding='utf8')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0904 03:25:18.093295 140496751892352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0904 03:25:18.141071 140496751892352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0904 03:25:18.201928 140496751892352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0904 03:25:18.204479 140496751892352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0904 03:25:18.215710 140496751892352 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0904 03:25:18.251626 140496751892352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","W0904 03:26:38.143629 140496751892352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","model_2 (Model)                 (None, None, 768)    101677056   input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 768)          0           model_2[1][0]                    \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 119)          91511       lambda_1[0][0]                   \n","==================================================================================================\n","Total params: 101,768,567\n","Trainable params: 101,768,567\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","stack:1/5\n"],"name":"stdout"},{"output_type":"stream","text":["W0904 03:27:02.100062 140496751892352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 39917 samples, validate on 10039 samples\n","Epoch 1/1\n","39917/39917 [==============================] - 5819s 146ms/step - loss: 1.7125 - acc: 0.5768 - val_loss: 1.4020 - val_acc: 0.6349\n","stack:2/5\n","Train on 39942 samples, validate on 10014 samples\n","Epoch 1/1\n"," 4930/39942 [==>...........................] - ETA: 1:18:51 - loss: 1.3166 - acc: 0.6485"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CKNvULaqho1Z","colab_type":"code","colab":{}},"source":["##获取第一第二个标签：取概率最大的前两个即可：\n","m=pd.DataFrame(stack_train)\n","first=[]\n","second=[]\n","for j,row in m.iterrows():\n","    zz=list(np.argsort(row))\n","    first.append(row.index[zz[-1]]) ##第一个标签\n","    second.append(row.index[zz[-2]]) ##第二个标签\n","m['label1']=first\n","m['label2']=second\n","\n","#计算准确率，只要命中一个就算正确：\n","k=0\n","for i in range(len(label)):\n","    if label[i] in [m.loc[i,'label1'],m.loc[i,'label2']]:\n","        k+=1\n","    else:\n","        pass\n","print('线下准确率：%f'%(k/len(label)))\n","\n","##准备测试集结果：\n","results=pd.DataFrame(stack_test)\n","first=[]\n","second=[]\n","for j,row in results.iterrows():\n","    zz=list(np.argsort(row))\n","    first.append(row.index[zz[-1]]) ##第一个标签\n","    second.append(row.index[zz[-2]]) ##第二个标签\n","results['label1']=first\n","results['label2']=second\n","##之前编码，最后逆编码回来：\n","\n","try:\n","    results['label1']=results['label1'].apply(lambda x:lbl.inverse_transform(int(x)))\n","    results['label2']=results['label2'].apply(lambda x:lbl.inverse_transform(int(x)))\n","except:\n","    results['label1']=lbl.inverse_transform(results['label1'])\n","    results['label2']=lbl.inverse_transform(results['label2'])\n","##结合id列，保存：\n","pd.concat([test[['id']],results[['label1','label2']]],axis=1).to_csv(path_drive+'submit2.csv',index=None,encoding='utf8')"],"execution_count":0,"outputs":[]}]}